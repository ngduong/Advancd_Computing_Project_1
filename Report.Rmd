---
title: "P8160 Project 1: Simulation Study for Variable Selection Methods"
author: "Ngoc Duong, Crystal Li, Yuchen Qi"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(MASS)
library(matrixcalc)
library(pracma)

library(dplyr)

library(ggplot2)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Introduction

Variables  selection methods are always trying to balance between model fitness and model complexity in the high-dimensional setting. And in application of traditional variables selection methods, they often struggle with identifying weak signals. Some signals are weak but they are still of importance to the true model.


# Objectives

In this project, we use simulations to compare two automated methods of variable selection, namely stepwise forward method and LASSO regression, in their ability to correctly identify relevant signals and estimating how missing weak signals impact coefficients of strong signals. Specifically, we aim to assess:  
(1) How well each method performs in identifying weak and strong predictors (by calculating the percentages of strong and weak predictors being captured by each model), and  
(2) How missing “weak” predictors impact the estimations of strong predictors (by calculating the bias and MSE between “true” strong coefficients and their estimates).


# Statistical methods to be studied

Methods of interest in this report are the step-wise forward method and automated LASSO regression which are two popular methods for the variable selection.

\begin{description}
\item[Step-wise forward method:] Starting with the empty model, and iteratively adds the variables that best improves the model fit. In this report, it is done by sequentially adding predictors with the largest reduction in AIC, where
$$AIC = n\ln(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n) + 2p,$$ where $\widehat{y}_i$ is the fitted values from a model, and $p$ is the dimension of the model (i.e.,number of predictors plus 1).



\item[Automated LASSO regression] It estimates the model parameters by optimizing a penalized loss function:
$$\min_\beta \frac{1}{2n} \sum_{i=1}^n (y_i - x_i \beta )^2 + \lambda \lVert \sum_{k=1}^p|\beta_k|$$
where $\lambda$ is a tunning parameter. Here cross-validation (CV) is the chosen selection criteria for LASSO.
\end{description} 


# Scenarios to be investigated

First we give the definitions of "strong", "weak-but-correlated” and “weak-andindependent” signals.  

Definition of strong signals --- 
$$S_1=\{j:|\beta_j|>c\sqrt{log (p) / n},\mbox{ some } c>0,  1\le j \le p\}$$
Definition of weak-but-correlated signals  ---
$$S_2=\{j: 0<|\beta_j|\le c\sqrt{log (p) / n},\mbox{ some } c>0, \mbox{corr}(X_j, X_j')\ne 0, \mbox{for some } j'\in S_1,  1\le j \le p\}$$
Definition of weak-and-independent signals  ---
$$S_3=\{j: 0<|\beta_j|\le c\sqrt{log (p) / n},\mbox{ some } c>0, \mbox{corr}(X_j, X_j')= 0, \mbox{for all } j'\in S_1,  1\le j \le p\}$$

To narrow the scope of our simulations, some variables are fixed.  
  (1) We set the proportions of strong signals, weak and independent signals, and weak but correlated signals to be 10%, 20%, 20% respectively, then we have 50% null predictors.  
  (2) The coefficients of strong signals follow Uniform(5, 10) which is sufficiently larger than the bound, and the coefficients of strong signals follow Uniform(1/2bound, bound), where the bound is threshold by definition.  
  (3) The threshold multiplier c is set to be 1.

## Task 1

* Case 1: we let the number of parameters vary from 10 to 100, with step size of 10 (low-dimension to high-dimension while still satisfying our n>p condition). In addition, we varied the correlation between the strong predictors and WBC predictors at 0.3, 0.5, and 0.7. The sample size is fixed at 100.
Scenario 2

* Case 2: e inspected both methods’ performances by letting the number of parameters larger than sample size. We let p vary at 100, 150, and 200, and the correlation between strong predictors and WBC predictors be 0.3 and 0.7. Our sample size is fixed at 90.

## Task 2

Here, we fixed the sample size at 100, the total number of parameters at 50. Our correlation coefficient is set to vary at 0.3 and 0.7.
 

# Methods for generating data

## Generating the predictor data matrix X

From the proportions of each type of signals and the number of total predictors, we get how many signals for each type. Then we generate a covariance matrix with the correlations set in this scenario following the definitions of each signal type. Whether the matrix is positive definite is also checked before passing it to the R function `mvrnorm`, which produces random numbers from a multivariate normal distribution. 

## Generating the response Y

We generate the response Y as a linear combination of four types of signals and an error term. The distribution of Y is  
$$Y\sim N(\boldsymbol X\boldsymbol \beta, \sigma^2)$$  
where the variance is 1.


# Performance Measures

## Task 1: identify strong and weak predictors

We wanted to investigate both variable selection methods’ ability to correctly identify strong and weak (both WAI and WBC) predictors and whether they do so consistently. Therefore, we measure their performances by calculating the percentages of captured strong, WBC and WAI predictors using these two methods as the number of parameters and correlation value changes.

## Task 2; how missing “weak” predictors impacts the estimations of strong predictor

In order to see the effect of missing weak predictors on the coefficient estimates of strong predictors, before fitting the models, we deleted a certain number of weak signals (from 1 to 20) from the original data. We then calculated the MSE and bias between “true” strong coefficients and their estimates, where  

* bias

$$\frac{1}{p_{strong}}\sum_{j=1}^{p_{strong}}(\hat{\boldsymbol\beta_j}-\boldsymbol\beta_j)$$

* MSE

$$\frac{1}{p_{strong}}\sum_{j=1}^{p_{strong}}(\hat{\boldsymbol\beta_j}-\boldsymbol\beta_j)^2$$


# Simulation results







```{r echo=FALSE, out.width='100%', fig.cap="Percent of captured Signals"}
knitr::include_graphics('F1.png')
```




# Code

The gitbub link is https://github.com/qi-yuchen/Advancd_Computing_Project_1.
